# Cluster configs for each environment test
  default-cluster-spec: &default-cluster-spec
    spark_version: '11.3.x-cpu-ml-scala2.12'
    node_type_id: 'Standard_DS3_v2' # NOTE: this is an AWS-specific instance type. Change accordingly if running on Azure or GCP.
    driver_node_type_id: 'Standard_DS3_v2'  # NOTE: this is an AWS-specific instance type. Change accordingly if running on Azure or GCP.
    num_workers: 1
    # To reduce start up time for each job, it is advisable to use a cluster pool. To do so involves supplying the following
    # two fields with a pool_id to acquire both the driver and instances from.
    # If driver_instance_pool_id and instance_pool_id are set, both node_type_id and driver_node_type_id CANNOT be supplied.
    # As such, if providing a pool_id for driver and worker instances, please ensure that node_type_id and driver_node_type_id are not present
    # driver_instance_pool_id: '0306-154610-mines187-pool-k93fghk'
    # instance_pool_id: '0306-154610-mines187-pool-k93fghke'

  prod-cluster-spec: &prod-cluster-spec
    spark_version: '11.3.x-cpu-ml-scala2.12'
    node_type_id: 'Standard_DS3_v2' # NOTE: this is an AWS-specific instance type. Change accordingly if running on Azure or GCP.
    driver_node_type_id: 'Standard_DS3_v2'  # NOTE: this is an AWS-specific instance type. Change accordingly if running on Azure or GCP.
    num_workers: 1
    # To reduce start up time for each job, it is advisable to use a cluster pool. To do so involves supplying the following
    # two fields with a pool_id to acquire both the driver and instances from.
    # If driver_instance_pool_id and instance_pool_id are set, both node_type_id and driver_node_type_id CANNOT be supplied.
    # As such, if providing a pool_id for driver and worker instances, please ensure that node_type_id and driver_node_type_id are not present
    # driver_instance_pool_id: '0306-154610-mines187-pool-k93fghke'
    # instance_pool_id: '0306-154610-mines187-pool-k93fghke'

  dev-cluster-config: &dev-cluster-config
    new_cluster:
      <<: *default-cluster-spec

  staging-cluster-config: &staging-cluster-config
    new_cluster:
      <<: *default-cluster-spec

  prod-cluster-config: &prod-cluster-config
    new_cluster:
      <<: *prod-cluster-spec


# Databricks Jobs definitions
# please note that we're using FUSE reference for config, and env files, hence we're going to load this file using its local FS path
environments:

  dev:
    strict_path_adjustment_policy: true
    workflows:
      - name: 'DEV-anomaly_detection-demo-setup'
        <<: *dev-cluster-config
        notebook_task:
          notebook_path: '/Repos/omar.hamdan@databricks.com/anomaly_detection/notebooks/demo_setup'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/demo_setup.yml']
      - name: 'DEV-anomaly_detection-feature-table-creation'
        <<: *dev-cluster-config
        notebook_task:
          notebook_path: '/Repos/omar.hamdan@databricks.com/anomaly_detection/notebooks/feature_table_creator'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/feature_table_creator.yml']
      - name: 'DEV-anomaly_detection-model-train'
        <<:
          - *dev-cluster-config
        notebook_task:
          notebook_path: '/Repos/omar.hamdan@databricks.com/anomaly_detection/notebooks/model_train'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/model_train.yml']
      - name: 'DEV-anomaly_detection-model-deployment'
        <<:
          - *dev-cluster-config
        notebook_task:
          notebook_path: '/Repos/omar.hamdan@databricks.com/anomaly_detection/notebooks/model_deployment'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/model_deployment.yml']
      - name: 'DEV-anomaly_detection-model-inference-batch'
        <<:
          - *dev-cluster-config
        notebook_task:
          notebook_path: '/Repos/omar.hamdan@databricks.com/anomaly_detection/notebooks/model_inference_batch'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/model_inference_batch.yml']
      - name: 'DEV-anomaly_detection-sample-integration-test'
        <<:
          - *dev-cluster-config
        notebook_task:
          notebook_path: '/Repos/omar.hamdan@databricks.com/anomaly_detection/tests/integration/sample_test.py'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/dev/.dev.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/sample_test.yml']

  staging:
    strict_path_adjustment_policy: true
    workflows:
      - name: 'STAGING-anomaly_detection-sample-integration-test'
        <<:
          - *staging-cluster-config
        spark_python_task:
          python_file: '/Repos/omar.hamdan@databricks.com/anomaly_detection/tests/integration/sample_test.py'
          parameters: ['--env', 'file:fuse://conf/staging/.staging.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/sample_test.yml']

  prod:
    strict_path_adjustment_policy: true
    workflows:
      - name: 'PROD-anomaly_detection-demo-setup'
        <<: *prod-cluster-config
        git_source:
          git_url: https://github.com/ohamdan77/anomaly_detection.git
          git_provider: "github"
          git_branch: "main"
        notebook_task:
          notebook_path: '/Repos/omar.hamdan@databricks.com/anomaly_detection/notebooks/demo_setup'
          parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                       '--env', 'file:fuse://conf/prod/.prod.env',
                       '--conf-file', 'file:fuse://conf/pipeline_configs/demo_setup.yml']

      - name: 'PROD-anomaly_detection-initial-model-train-register'
        tasks:
          - task_key: 'demo-setup'
            <<:
              - *prod-cluster-config
            spark_python_task:
              python_file: '/Repos/omar.hamdan@databricks.com/anomaly_detection/anomaly_detection/pipelines/demo_setup_job.py'
              parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                           '--env', 'file:fuse://conf/prod/.prod.env',
                           '--conf-file', 'file:fuse://conf/pipeline_configs/demo_setup.yml']
          - task_key: 'feature-table-creation'
            <<: *prod-cluster-config
            depends_on:
              - task_key: 'demo-setup'
            spark_python_task:
              python_file: '/Repos/omar.hamdan@databricks.com/anomaly_detection/anomaly_detection/pipelines/feature_table_creator_job.py'
              parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                           '--env', 'file:fuse://conf/prod/.prod.env',
                           '--conf-file', 'file:fuse://conf/pipeline_configs/feature_table_creator.yml']
          - task_key: 'model-train'
            <<: *prod-cluster-config
            depends_on:
              - task_key: 'demo-setup'
              - task_key: 'feature-table-creation'
            spark_python_task:
              python_file: '/Repos/omar.hamdan@databricks.com/anomaly_detection/anomaly_detection/pipelines/model_train_job.py'
              parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                           '--env', 'file:fuse://conf/prod/.prod.env',
                           '--conf-file', 'file:fuse://conf/pipeline_configs/model_train.yml']

      - name: 'PROD-anomaly_detection-train-deploy-inference-workflow'
        git_source:
          git_url: https://github.com/ohamdan77/anomaly_detection.git
          git_provider: "github"
          git_branch: "main"
        tasks:
          - task_key: 'model-train'
            <<: *prod-cluster-config
            notebook_task:
              notebook_path: 'notebooks/model_train'
              base_parameters: {"env" : "prod"}
            
          - task_key: 'model-deploy'
            <<: *prod-cluster-config
            notebook_task:
              notebook_path: 'notebooks/model_deployment'
              base_parameters: {"env" : "prod"}
            depends_on:
              - task_key: 'model-train'

          - task_key: 'model-inference-batch'
            <<: *prod-cluster-config
            notebook_task:
              notebook_path: 'notebooks/model_inference_batch'
              base_parameters: {"env" : "prod"}
            depends_on:
              - task_key: 'model-train'
              - task_key: 'model-deploy'

      - name: 'PROD-anomaly_detection-model-train'
        <<:
          - *prod-cluster-config
        git_source:
          git_url: https://github.com/ohamdan77/anomaly_detection.git
          git_provider: "github"
          git_branch: "main"
        tasks:
          - task_key: 'model-train'
            notebook_task:
              notebook_path: 'notebooks/model_train'
              base_parameters: {"env" : "prod"}
              parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                           '--env', 'file:fuse://conf/prod/.prod.env',
                           '--conf-file', 'file:fuse://conf/pipeline_configs/model_train.yml']
      - name: 'PROD-anomaly_detection-model-deployment'
        <<:
          - *prod-cluster-config
        git_source:
          git_url: https://github.com/ohamdan77/anomaly_detection.git
          git_provider: "github"
          git_branch: "main"
        tasks:
          - task_key: 'model-deploy'
            notebook_task:
              notebook_path: 'notebooks/model_deployment'
              parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                           '--env', 'file:fuse://conf/prod/.prod.env',
                           '--conf-file', 'file:fuse://conf/pipeline_configs/model_deployment.yml']
      - name: 'PROD-anomaly_detection-model-inference-batch'
        <<:
          - *prod-cluster-config
        git_source:
          git_url: https://github.com/ohamdan77/anomaly_detection.git
          git_provider: "github"
          git_branch: "main"
        tasks:
          - task_key: 'model-inference-batch'
            notebook_task:
              notebook_path: 'notebooks/model_inference_batch'
              parameters: ['--base-data-params', 'file:fuse://conf/.base_data_params.env',
                           '--env', 'file:fuse://conf/prod/.prod.env',
                           '--conf-file', 'file:fuse://conf/pipeline_configs/model_inference_batch.yml']
# environments:
#   default:
#     workflows:
#       #######################################################################################
#       #   Example workflow for integration tests                                            #
#       #######################################################################################
#       - name: "Anomaly_detection-sample-tests"
#         tasks:
#           - task_key: "main"
#             <<: *basic-static-cluster
#             spark_python_task:
#                 python_file: "file://tests/entrypoint.py"
#                 # this call supports all standard pytest arguments
#                 parameters: ["file:fuse://tests/integration", "--cov=anomaly_detection"]
#       #######################################################################################
#       # this is an example job with single ETL task based on 2.1 API and wheel_task format #
#       ######################################################################################
#       - name: "Anomaly_detection-sample-etl"
#         tasks:
#           - task_key: "main"
#             <<: *basic-static-cluster
#             python_wheel_task:
#               package_name: "anomaly_detection"
#               entry_point: "etl" # take a look at the setup.py entry_points section for details on how to define an entrypoint
#               parameters: ["--conf-file", "file:fuse://conf/tasks/sample_etl_config.yml"]
#       #############################################################
#       # this is an example multitask job with notebook task       #
#       #############################################################
#       - name: "Anomaly_detection-sample-multitask"
#         job_clusters:
#           - job_cluster_key: "default"
#             <<: *basic-static-cluster
#         tasks:
#           - task_key: "etl"
#             job_cluster_key: "default"
#             spark_python_task:
#               python_file: "file://anomaly_detection/tasks/sample_etl_task.py"
#               parameters: [ "--conf-file", "file:fuse://conf/tasks/sample_etl_config.yml" ]
#           - task_key: "ml"
#             depends_on:
#               - task_key: "etl"
#             job_cluster_key: "default"
#             python_wheel_task:
#               package_name: "anomaly_detection"
#               entry_point: "ml"
#               parameters: [ "--conf-file", "file:fuse://conf/tasks/sample_ml_config.yml" ]
#           ###############################################################################
#           # this is an example task based on the notebook                               #
#           # Please note that first you'll need to add a Repo and commit notebook to it. #
#           ###############################################################################
#           - task_key: "notebook"
#             deployment_config:
#               no_package: true # we omit using package since code will be shipped directly from the Repo
#             depends_on:
#               - task_key: "ml"
#             job_cluster_key: "default"
#             notebook_task:
#               notebook_path: "/Repos/Staging/anomaly_detection/notebooks/sample_notebook"

